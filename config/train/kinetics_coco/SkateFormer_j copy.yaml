seed: 1
num_worker: 4  # Balance between memory and performance with lazy loading
work_dir: ./work_dir/kinetics/coco/SkateFormer_j/

phase: train

# feeder - converted from OpenPose-18 via tools/convert_kinetics_openpose_to_coco.py
feeder: feeders.feeder_kinetics_coco.Feeder
train_feeder_args:
  data_path: /data/kinetics-skeleton
  split: train
  debug: False
  window_size: 64
  p_interval: [0.5, 1]
  aug_method: a123489p  # add 'p' to enable COCO part-drop when partition: True
  intra_p: 0.5
  inter_p: 0.0
  thres: 64
  uniform: True
  partition: False
  max_shards: -1  # Load all shards with lazy loading
  lazy_load: True  # Enable lazy loading to avoid OOM

test_feeder_args:
  data_path: /data/kinetics-skeleton
  split: test
  window_size: 64
  p_interval: [0.95]
  thres: 64
  uniform: True
  partition: False
  debug: False

# model
model: model.SkateFormer.SkateFormer_
model_args:
  num_classes: 400
  num_people: 2
  num_points: 17
  kernel_size: 7
  num_heads: 32
  attn_drop: 0.5
  head_drop: 0.0
  rel: True
  drop_path: 0.2
  type_1_size: [8, 17]
  type_2_size: [8, 17]
  type_3_size: [8, 17]
  type_4_size: [8, 17]
  mlp_ratio: 4.0
  index_t: True

# optim
optimizer: AdamW
weight_decay: 0.1
lr_scheduler: cosine
base_lr: 1e-3
min_lr: 1e-5
warmup_lr: 1e-7
warmup_prefix: False
warm_up_epoch: 25

# training
device: [0]
batch_size: 64
test_batch_size: 64
num_epoch: 300
nesterov: True
grad_clip: True
grad_max: 1.0
loss_type: LSCE

# model saving (auto save enabled)
save_interval: 5
save_epoch: 1

# performance optimizations
amp_enabled: True
compile_model: False  # Disabled: causes training hang on GPUs with few SMs
